{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": "        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        if (typeof require !== 'undefined') {\n        require.undef(\"plotly\");\n        requirejs.config({\n            paths: {\n                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n            }\n        });\n        require(['plotly'], function(Plotly) {\n            window._Plotly = Plotly;\n        });\n        }\n        </script>\n        "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import tensorflow as tf\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "cf.set_config_file(offline=False, world_readable=True)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import io\n",
    "import time\n",
    "from io import StringIO\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from PIL import *\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from ipywidgets import FileUpload\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\siwar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "FileUpload(value={}, accept='.csv', description='Upload', multiple=True)",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bc7b6e245f7e4856b2d075462ae13d3c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "upload = FileUpload(accept='.csv', multiple=True)\n",
    "display(upload)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "         Date                   Country  Cumulative no. of cases  \\\n0  2009-04-24                    Mexico                       18   \n1  2009-04-24  United States of America                        7   \n2  2009-04-26                    Mexico                       18   \n3  2009-04-26  United States of America                       20   \n4  2009-04-27                    Canada                        6   \n5  2009-04-27                    Mexico                       26   \n6  2009-04-27                     Spain                        1   \n7  2009-04-27  United States of America                       40   \n8  2009-04-28                    Canada                        6   \n9  2009-04-28                    Israel                        2   \n\n   Cumulative no. of deaths                                        Link  \n0                         0  https://www.who.int/csr/don/2009_04_24/en/  \n1                         0  https://www.who.int/csr/don/2009_04_24/en/  \n2                         0  https://www.who.int/csr/don/2009_04_24/en/  \n3                         0  https://www.who.int/csr/don/2009_04_24/en/  \n4                         0  https://www.who.int/csr/don/2009_04_24/en/  \n5                         7  https://www.who.int/csr/don/2009_04_24/en/  \n6                         0  https://www.who.int/csr/don/2009_04_24/en/  \n7                         0  https://www.who.int/csr/don/2009_04_24/en/  \n8                         0  https://www.who.int/csr/don/2009_04_24/en/  \n9                         0  https://www.who.int/csr/don/2009_04_24/en/  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Country</th>\n      <th>Cumulative no. of cases</th>\n      <th>Cumulative no. of deaths</th>\n      <th>Link</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2009-04-24</td>\n      <td>Mexico</td>\n      <td>18</td>\n      <td>0</td>\n      <td>https://www.who.int/csr/don/2009_04_24/en/</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2009-04-24</td>\n      <td>United States of America</td>\n      <td>7</td>\n      <td>0</td>\n      <td>https://www.who.int/csr/don/2009_04_24/en/</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2009-04-26</td>\n      <td>Mexico</td>\n      <td>18</td>\n      <td>0</td>\n      <td>https://www.who.int/csr/don/2009_04_24/en/</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2009-04-26</td>\n      <td>United States of America</td>\n      <td>20</td>\n      <td>0</td>\n      <td>https://www.who.int/csr/don/2009_04_24/en/</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2009-04-27</td>\n      <td>Canada</td>\n      <td>6</td>\n      <td>0</td>\n      <td>https://www.who.int/csr/don/2009_04_24/en/</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2009-04-27</td>\n      <td>Mexico</td>\n      <td>26</td>\n      <td>7</td>\n      <td>https://www.who.int/csr/don/2009_04_24/en/</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2009-04-27</td>\n      <td>Spain</td>\n      <td>1</td>\n      <td>0</td>\n      <td>https://www.who.int/csr/don/2009_04_24/en/</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2009-04-27</td>\n      <td>United States of America</td>\n      <td>40</td>\n      <td>0</td>\n      <td>https://www.who.int/csr/don/2009_04_24/en/</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2009-04-28</td>\n      <td>Canada</td>\n      <td>6</td>\n      <td>0</td>\n      <td>https://www.who.int/csr/don/2009_04_24/en/</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2009-04-28</td>\n      <td>Israel</td>\n      <td>2</td>\n      <td>0</td>\n      <td>https://www.who.int/csr/don/2009_04_24/en/</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=pd.read_csv(r'C:\\Users\\siwar\\Downloads\\datasetsMedical (1)\\datasetsMedical\\h1n1\\708381_1340787_bundle_archive\\data.csv')\n",
    "\n",
    "df1.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2490 entries, 0 to 2489\n",
      "Data columns (total 3 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   Cumulative no. of cases   2490 non-null   int64 \n",
      " 1   Cumulative no. of deaths  2490 non-null   int64 \n",
      " 2   Link                      2490 non-null   object\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 58.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df1.drop(['Date', 'Country'],axis=1,inplace=True)\n",
    "df1.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "   Cumulative no. of cases  Cumulative no. of deaths  \\\n0                       18                         0   \n1                        7                         0   \n2                       18                         0   \n3                       20                         0   \n4                        6                         0   \n\n                                         Link           label  \n0  https://www.who.int/csr/don/2009_04_24/en/  conjunctivitis  \n1  https://www.who.int/csr/don/2009_04_24/en/  conjunctivitis  \n2  https://www.who.int/csr/don/2009_04_24/en/  conjunctivitis  \n3  https://www.who.int/csr/don/2009_04_24/en/  conjunctivitis  \n4  https://www.who.int/csr/don/2009_04_24/en/  conjunctivitis  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cumulative no. of cases</th>\n      <th>Cumulative no. of deaths</th>\n      <th>Link</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>18</td>\n      <td>0</td>\n      <td>https://www.who.int/csr/don/2009_04_24/en/</td>\n      <td>conjunctivitis</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7</td>\n      <td>0</td>\n      <td>https://www.who.int/csr/don/2009_04_24/en/</td>\n      <td>conjunctivitis</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>18</td>\n      <td>0</td>\n      <td>https://www.who.int/csr/don/2009_04_24/en/</td>\n      <td>conjunctivitis</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20</td>\n      <td>0</td>\n      <td>https://www.who.int/csr/don/2009_04_24/en/</td>\n      <td>conjunctivitis</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6</td>\n      <td>0</td>\n      <td>https://www.who.int/csr/don/2009_04_24/en/</td>\n      <td>conjunctivitis</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['label'] = 'conjunctivitis'\n",
    "df1.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-7898b6de79b7>:19: FutureWarning:\n",
      "\n",
      "The default value of regex will change from True to False in a future version.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df1.reset_index(drop=True)\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "\n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower()  # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text)# replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing.\n",
    "    #text = re.sub(r'\\W+', '', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    return text\n",
    "df1['Link'] = df1['Link'].apply(clean_text)\n",
    "df1['Link'] = df1['Link'].str.replace('\\d+', '')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "   Cumulative no. of cases  Cumulative no. of deaths  \\\n0                       18                         0   \n1                        7                         0   \n2                       18                         0   \n3                       20                         0   \n4                        6                         0   \n\n                        Link           label  \n0  https wwwwhoint csr __ en  conjunctivitis  \n1  https wwwwhoint csr __ en  conjunctivitis  \n2  https wwwwhoint csr __ en  conjunctivitis  \n3  https wwwwhoint csr __ en  conjunctivitis  \n4  https wwwwhoint csr __ en  conjunctivitis  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cumulative no. of cases</th>\n      <th>Cumulative no. of deaths</th>\n      <th>Link</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>18</td>\n      <td>0</td>\n      <td>https wwwwhoint csr __ en</td>\n      <td>conjunctivitis</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7</td>\n      <td>0</td>\n      <td>https wwwwhoint csr __ en</td>\n      <td>conjunctivitis</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>18</td>\n      <td>0</td>\n      <td>https wwwwhoint csr __ en</td>\n      <td>conjunctivitis</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20</td>\n      <td>0</td>\n      <td>https wwwwhoint csr __ en</td>\n      <td>conjunctivitis</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6</td>\n      <td>0</td>\n      <td>https wwwwhoint csr __ en</td>\n      <td>conjunctivitis</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "class TextRank4Keyword():\n",
    "    \"\"\"Extract keywords from text\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.d = 0.85 # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5 # convergence threshold\n",
    "        self.steps = 10 # iteration steps\n",
    "        self.node_weight = None # save keywords and its weight\n",
    "\n",
    "\n",
    "    def set_stopwords(self, stopwords):\n",
    "        \"\"\"Set stop words\"\"\"\n",
    "        for word in STOP_WORDS.union(set(stopwords)):\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "\n",
    "    def sentence_segment(self, doc, candidate_pos, lower):\n",
    "        \"\"\"Store those words only in cadidate_pos\"\"\"\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            selected_words = []\n",
    "            for token in sent:\n",
    "                # Store words only with cadidate POS tag\n",
    "                if token.pos_ in candidate_pos and token.is_stop is False:\n",
    "                    if lower is True:\n",
    "                        selected_words.append(token.text.lower())\n",
    "                    else:\n",
    "                        selected_words.append(token.text)\n",
    "            sentences.append(selected_words)\n",
    "        return sentences\n",
    "\n",
    "    def get_vocab(self, sentences):\n",
    "        \"\"\"Get all tokens\"\"\"\n",
    "        vocab = OrderedDict()\n",
    "        i = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = i\n",
    "                    i += 1\n",
    "        return vocab\n",
    "\n",
    "    def get_token_pairs(self, window_size, sentences):\n",
    "        \"\"\"Build token_pairs from windows in sentences\"\"\"\n",
    "        token_pairs = list()\n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for j in range(i+1, i+window_size):\n",
    "                    if j >= len(sentence):\n",
    "                        break\n",
    "                    pair = (word, sentence[j])\n",
    "                    if pair not in token_pairs:\n",
    "                        token_pairs.append(pair)\n",
    "        return token_pairs\n",
    "\n",
    "    def symmetrize(self, a):\n",
    "        return a + a.T - np.diag(a.diagonal())\n",
    "\n",
    "    def get_matrix(self, vocab, token_pairs):\n",
    "        \"\"\"Get normalized matrix\"\"\"\n",
    "        # Build matrix\n",
    "        vocab_size = len(vocab)\n",
    "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "        for word1, word2 in token_pairs:\n",
    "            i, j = vocab[word1], vocab[word2]\n",
    "            g[i][j] = 1\n",
    "\n",
    "        # Get Symmeric matrix\n",
    "        g = self.symmetrize(g)\n",
    "\n",
    "        # Normalize matrix by column\n",
    "        norm = np.sum(g, axis=0)\n",
    "        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n",
    "\n",
    "        return g_norm\n",
    "\n",
    "\n",
    "    def get_keywords(self, number=10):\n",
    "        \"\"\"Print top number keywords\"\"\"\n",
    "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
    "        for i, (key, value) in enumerate(node_weight.items()):\n",
    "            print(key + ' - ' + str(value))\n",
    "            if i > number:\n",
    "                break\n",
    "\n",
    "\n",
    "    def analyze(self, text,\n",
    "                candidate_pos=['NOUN', 'PROPN'],\n",
    "                window_size=4, lower=False, stopwords=list()):\n",
    "        \"\"\"Main function to analyze text\"\"\"\n",
    "\n",
    "        # Set stop words\n",
    "        self.set_stopwords(stopwords)\n",
    "\n",
    "        # Pare text by spaCy\n",
    "        doc = nlp(text)\n",
    "\n",
    "        # Filter sentences\n",
    "        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n",
    "\n",
    "        # Build vocabulary\n",
    "        vocab = self.get_vocab(sentences)\n",
    "\n",
    "        # Get token_pairs from windows\n",
    "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
    "\n",
    "        # Get normalized matrix\n",
    "        g = self.get_matrix(vocab, token_pairs)\n",
    "\n",
    "        # Initionlization for weight(pagerank value)\n",
    "        pr = np.array([1] * len(vocab))\n",
    "\n",
    "        # Iteration\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr = (1-self.d) + self.d * np.dot(g, pr)\n",
    "            if abs(previous_pr - sum(pr))  < self.min_diff:\n",
    "                break\n",
    "            else:\n",
    "                previous_pr = sum(pr)\n",
    "\n",
    "        # Get weight for each node\n",
    "        node_weight = dict()\n",
    "        for word, index in vocab.items():\n",
    "            node_weight[word] = pr[index]\n",
    "\n",
    "        self.node_weight = node_weight"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-12-49a157387da0>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mdocs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdf1\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtolist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Python\\Python38\\lib\\site-packages\\pandas\\core\\generic.py\u001B[0m in \u001B[0;36m__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m   5463\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_info_axis\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_can_hold_identifiers_and_holds_name\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   5464\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 5465\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mobject\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__getattribute__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   5466\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   5467\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__setattr__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'DataFrame' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "docs = df1.text.values.tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "str1 = ''.join(docs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text = str1\n",
    "tr4w = TextRank4Keyword()\n",
    "tr4w.analyze(text, window_size=4, lower=False)\n",
    "tr4w.get_keywords()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "get_ipython().system('pip install yake')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import yake\n",
    "\n",
    "text = str1\n",
    "\n",
    "# assuming default parameters\n",
    "simple_kwextractor = yake.KeywordExtractor()\n",
    "keywords = simple_kwextractor.extract_keywords(text)\n",
    "\n",
    "for kw in keywords:\n",
    "\tprint(kw)\n",
    "\n",
    "# specifying parameters\n",
    "max_ngram_size = 1\n",
    "custom_kwextractor = yake.KeywordExtractor(lan=\"en\", n = max_ngram_size, dedupLim=0.9, dedupFunc='seqm', windowsSize=1,  features=None)\n",
    "\n",
    "keywords = custom_kwextractor.extract_keywords(text)\n",
    "\n",
    "for kw in keywords:\n",
    "\tprint(kw)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}